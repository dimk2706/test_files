import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import uuid
import json
import random
import os

def generate_random_data(num_rows=10):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    
    # –°–ø–∏—Å–∫–∏ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    symbols = ['CNY/RUB', 'USD/RUB', 'EUR/RUB', 'GBP/RUB', 'JPY/RUB']
    states = [0, 1]
    tenors = ['TOM', 'SPOT', 'TOD', 'ON']
    tiers = ['TRADER1', 'TRADER2', 'TRADER3', 'TRADER4', 'TRADER5']
    
    data = []
    
    for _ in range(num_rows):
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        time = datetime.now() - timedelta(days=random.randint(0, 30), 
                                         hours=random.randint(0, 23),
                                         minutes=random.randint(0, 59))
        ulid = str(uuid.uuid4())[:24].upper()
        symbol = random.choice(symbols)
        state = random.choice(states)
        tenor = random.choice(tenors)
        
        # Value date (–±–ª–∏–∂–∞–π—à–∞—è –¥–∞—Ç–∞)
        value_date_near = time + timedelta(days=random.randint(1, 5))
        
        # –§–ª–∞–≥–∏
        global_tradable = random.choice([0, 1])
        global_indicative = 1 - global_tradable  # –æ–±—ã—á–Ω–æ –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏–µ
        
        # Rate ID (—Å–ª—É—á–∞–π–Ω—ã–π –±–æ–ª—å—à–æ–π –Ω–æ–º–µ—Ä)
        rate_id = random.randint(10000000000, 99999999999)
        
        # Tier
        tier = random.choice(tiers)
        
        # Price levels (–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ü–µ–Ω—ã –∏ —Ä–∞–∑–º–µ—Ä—ã)
        bid_price = random.randint(8000000, 12000000)
        ask_price = bid_price + random.randint(100000, 500000)
        size = random.randint(100000, 5000000)
        
        price_levels = {
            'bid': {
                'price': str(bid_price),
                'size': str(size)
            },
            'ask': {
                'price': str(ask_price),
                'size': str(size)
            }
        }
        
        data.append({
            'time': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'ulid': ulid,
            'symbol': symbol,
            'state': state,
            'tenor': tenor,
            'valueDateNear': value_date_near.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'globalTradable': global_tradable,
            'globalIndicative': global_indicative,
            'rateId': rate_id,
            'tier': tier,
            'priceLevels': json.dumps(price_levels)
        })
    
    return data

def get_next_file_number(base_name="–ö–Ω–∏–≥–∞1", extension="xlsx"):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã"""
    today = datetime.now().strftime("%Y-%m-%d")
    pattern = f"{base_name}_{today}_"
    
    # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã —Å —ç—Ç–æ–π –¥–∞—Ç–æ–π
    existing_files = []
    for file in os.listdir('.'):
        if file.startswith(pattern) and file.endswith(f".{extension}"):
            existing_files.append(file)
    
    if not existing_files:
        return 1
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ –∏–∑ –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤
    numbers = []
    for file in existing_files:
        try:
            # –§–æ—Ä–º–∞—Ç: –ö–Ω–∏–≥–∞1_2024-01-15_3.xlsx
            number_part = file.replace(f"{pattern}", "").replace(f".{extension}", "")
            number = int(number_part)
            numbers.append(number)
        except ValueError:
            continue
    
    return max(numbers) + 1 if numbers else 1

def create_data_files(num_rows=10):
    """–°–æ–∑–¥–∞–µ—Ç Excel —Ñ–∞–π–ª –∏ Parquet –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç–æ–π –∏ –Ω–æ–º–µ—Ä–æ–º"""
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞
    file_number = get_next_file_number()
    today = datetime.now().strftime("%Y-%m-%d")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    data = generate_random_data(num_rows)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(data)
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
    df.columns = ['time', 'ulid', 'symbol', 'state', 'tenor', 'valueDateNear', 
                  'globalTradable', 'globalIndicative', 'rateId', 'tier', 'priceLevels']
    
    # –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
    excel_filename = f"–ö–Ω–∏–≥–∞1_{today}_{file_number}.xlsx"
    parquet_filename = f"database_{today}_{file_number}.parquet"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='–õ–∏—Å—Ç1', index=False)
        
        # –ü–æ–ª—É—á–∞–µ–º workbook –∏ worksheet –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        workbook = writer.book
        worksheet = writer.sheets['–õ–∏—Å—Ç1']
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        column_widths = {
            'A': 20,  # time
            'B': 30,  # ulid
            'C': 12,  # symbol
            'D': 8,   # state
            'E': 8,   # tenor
            'F': 20,  # valueDateNear
            'G': 15,  # globalTradable
            'H': 18,  # globalIndicative
            'I': 15,  # rateId
            'J': 10,  # tier
            'K': 50   # priceLevels
        }
        
        for col, width in column_widths.items():
            worksheet.column_dimensions[col].width = width
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
    df.to_parquet(parquet_filename, index=False, engine='pyarrow')
    
    print(f"‚úÖ Excel —Ñ–∞–π–ª '{excel_filename}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω —Å {num_rows} —Å—Ç—Ä–æ–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö")
    print(f"‚úÖ Parquet –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{parquet_filename}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞")
    
    return excel_filename, parquet_filename, df

def create_consolidated_database():
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö Parquet —Ñ–∞–π–ª–æ–≤"""
    parquet_files = [f for f in os.listdir('.') if f.startswith('database_') and f.endswith('.parquet')]
    
    if not parquet_files:
        print("‚ùå Parquet —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏")
        return None
    
    all_data = []
    for file in parquet_files:
        try:
            df = pd.read_parquet(file)
            df['source_file'] = file  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            all_data.append(df)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {file}: {e}")
    
    if all_data:
        consolidated_df = pd.concat(all_data, ignore_index=True)
        consolidated_filename = f"consolidated_database_{datetime.now().strftime('%Y-%m-%d')}.parquet"
        consolidated_df.to_parquet(consolidated_filename, index=False, engine='pyarrow')
        print(f"‚úÖ –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö '{consolidated_filename}' —Å–æ–∑–¥–∞–Ω–∞")
        print(f"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(parquet_files)} —Ñ–∞–π–ª–æ–≤, –≤—Å–µ–≥–æ {len(consolidated_df)} –∑–∞–ø–∏—Å–µ–π")
        return consolidated_filename
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
        return None

def read_and_display_parquet(filename):
    """–ß–∏—Ç–∞–µ—Ç –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Parquet —Ñ–∞–π–ª–∞"""
    try:
        df = pd.read_parquet(filename)
        print(f"\nüìä –î–∞–Ω–Ω—ã–µ –∏–∑ {filename}:")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        print("\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏:")
        print(df.head(3))
        return df
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ Parquet —Ñ–∞–π–ª–∞: {e}")
        return None

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã
    excel_file, parquet_file, df = create_data_files(150)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Parquet
    #read_and_display_parquet(parquet_file)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    print("\n" + "="*50)
    consolidated_file = create_consolidated_database()
    if consolidated_file:
        read_and_display_parquet(consolidated_file)
    